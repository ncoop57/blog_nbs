{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"FAQ-Style QA\": Utilizing existing FAQs for Question Answering\n",
    "\n",
    "While *extractive Question Answering* works on pure texts and is therefore more generalizable, there's also a common alternative that utilizes existing FAQ data.\n",
    "\n",
    "Pros:\n",
    "- Very fast at inference time\n",
    "- Utilize existing FAQ data\n",
    "- Quite good control over answers\n",
    "\n",
    "Cons:\n",
    "- Generalizability: We can only answer questions that are similar to existing ones in FAQ\n",
    "\n",
    "In some use cases, a combination of extractive QA and FAQ-style can also be an interesting option.\n",
    "\n",
    "*Use this [link](https://colab.research.google.com/github/deepset-ai/haystack/blob/master/tutorials/Tutorial4_Tutorial4_FAQ_style_QA.ipynb) to open the notebook in Google Colab.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "contents = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading arxiv.zip to /content/blog_nbs/scratch_nbs/google_scholar\n 99% 873M/885M [00:09<00:00, 112MB/s]\n100% 885M/885M [00:09<00:00, 99.6MB/s]\n"
    }
   ],
   "source": [
    "! kaggle datasets download 'Cornell-University/arxiv'\n",
    "! unzip -qq arxiv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data  = []\n",
    "with open(\"arxiv-metadata-oai-snapshot.json\", 'r') as f:\n",
    "    for line in f: \n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "kages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n\u001b[K     |████████████████████████████████| 778kB 38.7MB/s \n\u001b[?25hCollecting pydantic<2.0.0,>=1.0.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/5f/855412ad12817ae87f1c77d3af2fc384eaed3adfb8f3994816d75483fa20/pydantic-1.6.1-cp36-cp36m-manylinux2014_x86_64.whl (8.7MB)\n\u001b[K     |████████████████████████████████| 8.7MB 35.1MB/s \n\u001b[?25hCollecting starlette==0.13.6\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/a4/c9e228d7d47044ce4c83ba002f28ff479e542455f0499198a3f77c94f564/starlette-0.13.6-py3-none-any.whl (59kB)\n\u001b[K     |████████████████████████████████| 61kB 5.8MB/s \n\u001b[?25hCollecting httptools==0.1.*; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\"\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/a6/dc1e7e8f4049ab70d52c9690ec10652e268ab2542853033cc1d539594102/httptools-0.1.1-cp36-cp36m-manylinux1_x86_64.whl (216kB)\n\u001b[K     |████████████████████████████████| 225kB 36.8MB/s \n\u001b[?25hRequirement already satisfied: click==7.* in /usr/local/lib/python3.6/dist-packages (from uvicorn->farm-haystack==0.3.0) (7.1.2)\nCollecting uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\"\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/48/586225bbb02d3bdca475b17e4be5ce5b3f09da2d6979f359916c1592a687/uvloop-0.14.0-cp36-cp36m-manylinux2010_x86_64.whl (3.9MB)\n\u001b[K     |████████████████████████████████| 3.9MB 36.9MB/s \n\u001b[?25hCollecting websockets==8.*\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n\u001b[K     |████████████████████████████████| 81kB 8.4MB/s \n\u001b[?25hCollecting h11<0.10,>=0.8\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n\u001b[K     |████████████████████████████████| 61kB 6.6MB/s \n\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->farm-haystack==0.3.0) (2018.9)\nRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->farm-haystack==0.3.0) (2.8.1)\nRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->farm-haystack==0.3.0) (1.18.5)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->farm-haystack==0.3.0) (0.22.2.post1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from elasticsearch->farm-haystack==0.3.0) (2020.6.20)\nRequirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch->farm-haystack==0.3.0) (1.24.3)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tox->farm-haystack==0.3.0) (1.15.0)\nRequirement already satisfied: packaging>=14 in /usr/local/lib/python3.6/dist-packages (from tox->farm-haystack==0.3.0) (20.4)\nCollecting pluggy>=0.12.0\n  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\nRequirement already satisfied: importlib-metadata<2,>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from tox->farm-haystack==0.3.0) (1.7.0)\nCollecting virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/51/36c685ff2c1b2f7b4b5db29f3153159102ae0e0adaff3a26fd1448232e06/virtualenv-20.0.31-py2.py3-none-any.whl (4.9MB)\n\u001b[K     |████████████████████████████████| 4.9MB 41.8MB/s \n\u001b[?25hRequirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.6/dist-packages (from tox->farm-haystack==0.3.0) (0.10.1)\nRequirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from tox->farm-haystack==0.3.0) (3.0.12)\nRequirement already satisfied: py>=1.4.17 in /usr/local/lib/python3.6/dist-packages (from tox->farm-haystack==0.3.0) (1.9.0)\nRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx->farm-haystack==0.3.0) (4.2.6)\nRequirement already satisfied: SQLAlchemy>=1.0 in /usr/local/lib/python3.6/dist-packages (from sqlalchemy_utils->farm-haystack==0.3.0) (1.3.19)\nRequirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->farm==0.4.7->farm-haystack==0.3.0) (2.4.3)\nRequirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->farm==0.4.7->farm-haystack==0.3.0) (2.11.2)\nRequirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->farm==0.4.7->farm-haystack==0.3.0) (1.1.0)\nRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<=1.6.0,>=1.5.1->farm==0.4.7->farm-haystack==0.3.0) (0.16.0)\nRequirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from flask-restplus->farm==0.4.7->farm-haystack==0.3.0) (2.6.0)\nCollecting aniso8601>=0.82\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/e4/787e104b58eadc1a710738d4e418d7e599e4e778e52cb8e5d5ef6ddd5833/aniso8601-8.0.0-py2.py3-none-any.whl (43kB)\n\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.7->farm-haystack==0.3.0) (0.3)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.7->farm-haystack==0.3.0) (1.3.0)\nCollecting querystring-parser\n  Downloading https://files.pythonhosted.org/packages/4a/fa/f54f5662e0eababf0c49e92fd94bf178888562c0e7b677c8941bbbcd1bd6/querystring_parser-1.2.4.tar.gz\nRequirement already satisfied: sqlparse in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.7->farm-haystack==0.3.0) (0.3.1)\nRequirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.7->farm-haystack==0.3.0) (3.12.4)\nCollecting gitpython>=2.1.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/bc/ae32e07e89cc25b9e5c793d19a1e5454d30a8e37d95040991160f942519e/GitPython-3.1.8-py3-none-any.whl (159kB)\n\u001b[K     |████████████████████████████████| 163kB 46.4MB/s \n\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.7->farm-haystack==0.3.0) (3.13)\nCollecting alembic\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/aa/c261dfd7f4ba6ce4701846a2689a46e2a172e012171de4378fc2926e3bf0/alembic-1.4.3-py2.py3-none-any.whl (159kB)\n\u001b[K     |████████████████████████████████| 163kB 47.2MB/s \n\u001b[?25hCollecting databricks-cli>=0.8.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/57/5c2d6b83cb8753d12f548e89f91037632baa8289677c1b2ab2adf14bf6b2/databricks-cli-0.11.0.tar.gz (49kB)\n\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n\u001b[?25hCollecting docker>=3.6.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/8c/8d42dbd83679483db207535f4fb02dc84325fa78b290f057694b057fcd21/docker-4.3.1-py2.py3-none-any.whl (145kB)\n\u001b[K     |████████████████████████████████| 153kB 46.3MB/s \n\u001b[?25hCollecting simplejson\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/96/1e6b19045375890068d7342cbe280dd64ae73fd90b9735b5efb8d1e044a1/simplejson-3.17.2-cp36-cp36m-manylinux2010_x86_64.whl (127kB)\n\u001b[K     |████████████████████████████████| 133kB 45.8MB/s \n\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->farm==0.4.7->farm-haystack==0.3.0) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->farm==0.4.7->farm-haystack==0.3.0) (2.10)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->farm==0.4.7->farm-haystack==0.3.0) (0.3.3)\nRequirement already satisfied: botocore<1.18.0,>=1.17.59 in /usr/local/lib/python3.6/dist-packages (from boto3->farm==0.4.7->farm-haystack==0.3.0) (1.17.59)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->farm==0.4.7->farm-haystack==0.3.0) (0.10.0)\nCollecting sentencepiece!=0.1.92\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n\u001b[K     |████████████████████████████████| 1.1MB 46.5MB/s \n\u001b[?25hCollecting tokenizers==0.8.1.rc1\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n\u001b[K     |████████████████████████████████| 3.0MB 37.5MB/s \n\u001b[?25hCollecting sacremoses\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n\u001b[K     |████████████████████████████████| 890kB 33.7MB/s \n\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->farm==0.4.7->farm-haystack==0.3.0) (2019.12.20)\nRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->farm==0.4.7->farm-haystack==0.3.0) (0.7)\nRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->farm-haystack==0.3.0) (0.16.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=14->tox->farm-haystack==0.3.0) (2.4.7)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata<2,>=0.12; python_version < \"3.8\"->tox->farm-haystack==0.3.0) (3.1.0)\nCollecting appdirs<2,>=1.4.3\n  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\nCollecting importlib-resources>=1.0; python_version < \"3.7\"\n  Downloading https://files.pythonhosted.org/packages/ba/03/0f9595c0c2ef12590877f3c47e5f579759ce5caf817f8256d5dcbd8a1177/importlib_resources-3.0.0-py2.py3-none-any.whl\nCollecting distlib<1,>=0.3.1\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/0a/490fa011d699bb5a5f3a0cf57de82237f52a6db9d40f33c53b2736c9a1f9/distlib-0.3.1-py2.py3-none-any.whl (335kB)\n\u001b[K     |████████████████████████████████| 337kB 47.4MB/s \n\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->farm==0.4.7->farm-haystack==0.3.0) (2.10.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->farm==0.4.7->farm-haystack==0.3.0) (1.1.1)\nCollecting gitdb<5,>=4.0.1\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n\u001b[K     |████████████████████████████████| 71kB 8.0MB/s \n\u001b[?25hCollecting python-editor>=0.3\n  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\nCollecting Mako\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n\u001b[K     |████████████████████████████████| 81kB 8.6MB/s \n\u001b[?25hRequirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from databricks-cli>=0.8.0->mlflow==1.0.0->farm==0.4.7->farm-haystack==0.3.0) (0.8.7)\nCollecting websocket-client>=0.32.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n\u001b[K     |████████████████████████████████| 204kB 30.1MB/s \n\u001b[?25hRequirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.59->boto3->farm==0.4.7->farm-haystack==0.3.0) (0.15.2)\nCollecting smmap<4,>=3.0.1\n  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\nBuilding wheels for collected packages: farm-haystack, langdetect, wget, python-multipart, python-docx, sqlalchemy-utils, tika, seqeval, querystring-parser, databricks-cli, sacremoses\n  Building wheel for farm-haystack (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for farm-haystack: filename=farm_haystack-0.3.0-cp36-none-any.whl size=103212 sha256=906722388f55683bf9bb0befea49b9b8b6bc6ef79c4f3417535d5c48a4cd5289\n  Stored in directory: /tmp/pip-ephem-wheel-cache-roifhugc/wheels/ab/41/a4/4fbf362de283352078ecb6705c08b6525347aaea2eead2a60c\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=7912fc96134552acab63c76f5ba9a7be82c8a63c86551b08bcff974c85a7f2a1\n  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=3224c395d6eb896f4c03620202434ca3bab74dfeb3e486be817800033920e947\n  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for python-multipart: filename=python_multipart-0.0.5-cp36-none-any.whl size=31671 sha256=4da065ccfdd89d64d76a873b3ae0048be675322e37b04a7b5c27dd7dbd16a17d\n  Stored in directory: /root/.cache/pip/wheels/f0/e6/66/14a866a3cbd6a0cabfbef91f7edf40aa03595ef6c88d6d1be4\n  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for python-docx: filename=python_docx-0.8.10-cp36-none-any.whl size=184491 sha256=84671b0e3a67e4a6179aaad153dfed25f20d03623ee47dc56cad4ac41d768765\n  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n  Building wheel for sqlalchemy-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for sqlalchemy-utils: filename=SQLAlchemy_Utils-0.36.8-py2.py3-none-any.whl size=93219 sha256=7ea85975d3c61efbae0fe7eefa8c6fb982e46e8e4e7d3aca36d9f74a8472f1bb\n  Stored in directory: /root/.cache/pip/wheels/68/31/b6/a96bf6868f42753696d647846c9a0f8e51bd99295790d07660\n  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for tika: filename=tika-1.24-cp36-none-any.whl size=32884 sha256=42bf84b9bad845deecf291a8f7a144522ab07707df6ffa6e2dfdd568202d13e4\n  Stored in directory: /root/.cache/pip/wheels/73/9c/f5/0b1b738442fc2a2862bef95b908b374f8e80215550fb2a8975\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7423 sha256=317ba0d6ce3abab3be2750f23b6d18bceb9896e722ee0d163dc0bd191a12c24c\n  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n  Building wheel for querystring-parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for querystring-parser: filename=querystring_parser-1.2.4-cp36-none-any.whl size=7078 sha256=027028c3a174916b9ef121de3474f4c8f82a7205730cdbdf77819beaa29906f3\n  Stored in directory: /root/.cache/pip/wheels/1e/41/34/23ebf5d1089a9aed847951e0ee375426eb4ad0a7079d88d41e\n  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for databricks-cli: filename=databricks_cli-0.11.0-cp36-none-any.whl size=90301 sha256=2076d2952116a29d5de46b2fdb14242e6d97d7c5cae9d123ae9002ccd057d2d7\n  Stored in directory: /root/.cache/pip/wheels/63/d0/4f/3deeca1f4c47a6aca7c2c6a6e2bf272391565dc86a7718a59b\n  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=ac029f6abc5bab584df2366783acaa988c9a159afae7757b908bc32f3defc2d4\n  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\nSuccessfully built farm-haystack langdetect wget python-multipart python-docx sqlalchemy-utils tika seqeval querystring-parser databricks-cli sacremoses\n\u001b[31mERROR: pytest 3.6.4 has requirement pluggy<0.8,>=0.5, but you'll have pluggy 0.13.1 which is incompatible.\u001b[0m\n\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\nInstalling collected packages: flask-cors, seqeval, torch, dotmap, Werkzeug, aniso8601, flask-restplus, querystring-parser, gunicorn, smmap, gitdb, gitpython, python-editor, Mako, alembic, databricks-cli, websocket-client, docker, simplejson, mlflow, sentencepiece, tokenizers, sacremoses, transformers, farm, pydantic, starlette, fastapi, httptools, uvloop, websockets, h11, uvicorn, psycopg2-binary, elasticsearch, elastic-apm, pluggy, appdirs, importlib-resources, distlib, virtualenv, tox, langdetect, wget, python-multipart, python-docx, sqlalchemy-utils, faiss-cpu, tika, farm-haystack\n  Found existing installation: torch 1.6.0+cu101\n    Uninstalling torch-1.6.0+cu101:\n      Successfully uninstalled torch-1.6.0+cu101\n  Found existing installation: Werkzeug 1.0.1\n    Uninstalling Werkzeug-1.0.1:\n      Successfully uninstalled Werkzeug-1.0.1\n  Found existing installation: pluggy 0.7.1\n    Uninstalling pluggy-0.7.1:\n      Successfully uninstalled pluggy-0.7.1\nSuccessfully installed Mako-1.1.3 Werkzeug-0.16.1 alembic-1.4.3 aniso8601-8.0.0 appdirs-1.4.4 databricks-cli-0.11.0 distlib-0.3.1 docker-4.3.1 dotmap-1.3.0 elastic-apm-5.8.1 elasticsearch-7.9.1 faiss-cpu-1.6.3 farm-0.4.7 farm-haystack-0.3.0 fastapi-0.61.1 flask-cors-3.0.9 flask-restplus-0.13.0 gitdb-4.0.5 gitpython-3.1.8 gunicorn-20.0.4 h11-0.9.0 httptools-0.1.1 importlib-resources-3.0.0 langdetect-1.0.8 mlflow-1.0.0 pluggy-0.13.1 psycopg2-binary-2.8.6 pydantic-1.6.1 python-docx-0.8.10 python-editor-1.0.4 python-multipart-0.0.5 querystring-parser-1.2.4 sacremoses-0.0.43 sentencepiece-0.1.91 seqeval-0.0.12 simplejson-3.17.2 smmap-3.0.4 sqlalchemy-utils-0.36.8 starlette-0.13.6 tika-1.24 tokenizers-0.8.1rc1 torch-1.6.0 tox-3.20.0 transformers-3.0.2 uvicorn-0.11.8 uvloop-0.14.0 virtualenv-20.0.31 websocket-client-0.57.0 websockets-8.1 wget-3.2\nLooking in links: https://download.pytorch.org/whl/torch_stable.html\nCollecting torch==1.5.1+cu101\n\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n\u001b[K     |████████████████████████████████| 704.4MB 24kB/s \n\u001b[?25hCollecting torchvision==0.6.1+cu101\n\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n\u001b[K     |████████████████████████████████| 6.6MB 23.9MB/s \n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\nRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\nRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\nInstalling collected packages: torch, torchvision\n  Found existing installation: torch 1.6.0\n    Uninstalling torch-1.6.0:\n      Successfully uninstalled torch-1.6.0\n  Found existing installation: torchvision 0.7.0+cu101\n    Uninstalling torchvision-0.7.0+cu101:\n      Successfully uninstalled torchvision-0.7.0+cu101\nSuccessfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n"
    }
   ],
   "source": [
    "# Install the latest release of Haystack in your own environment \n",
    "#! pip install farm-haystack\n",
    "\n",
    "# Install the latest master of Haystack and install the version of torch that works with the colab GPUs\n",
    "!pip install git+https://github.com/deepset-ai/haystack.git\n",
    "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from haystack import Finder\n",
    "from haystack.database.elasticsearch import ElasticsearchDocumentStore\n",
    "\n",
    "from haystack.retriever.dense import EmbeddingRetriever\n",
    "from haystack.utils import print_answers\n",
    "import pandas as pd\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start an Elasticsearch server\n",
    "You can start Elasticsearch on your local machine instance using Docker. If Docker is not readily available in your environment (eg., in Colab notebooks), then you can manually download and execute Elasticsearch from source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended: Start Elasticsearch using Docker\n",
    "# ! docker run -d -p 9200:9200 -e \"discovery.type=single-node\" elasticsearch:7.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Colab / No Docker environments: Start Elasticsearch from source\n",
    "# ! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n",
    "# ! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n",
    "# ! chown -R daemon:daemon elasticsearch-7.6.2\n",
    "\n",
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],\n",
    "                   stdout=PIPE, stderr=STDOUT,\n",
    "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    "                  )\n",
    "# wait until ES has started\n",
    "! sleep 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Init the DocumentStore\n",
    "In contrast to Tutorial 1 (extractive QA), we:\n",
    "\n",
    "* specify the name of our `text_field` in Elasticsearch that we want to return as an answer\n",
    "* specify the name of our `embedding_field` in Elasticsearch where we'll store the embedding of our question and that is used later for calculating our similarity to the incoming user question\n",
    "* set `excluded_meta_data=[\"question_emb\"]` so that we don't return the huge embedding vectors in our search results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "09/16/2020 02:24:15 - INFO - elasticsearch -   HEAD http://localhost:9200/document [status:200 request:0.161s]\n09/16/2020 02:24:15 - INFO - elasticsearch -   HEAD http://localhost:9200/label [status:200 request:0.007s]\n"
    }
   ],
   "source": [
    "from haystack.database.elasticsearch import ElasticsearchDocumentStore\n",
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\",\n",
    "                                            index=\"document\",\n",
    "                                            embedding_field=\"question_emb\",\n",
    "                                            embedding_dim=768,\n",
    "                                            excluded_meta_data=[\"question_emb\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a Retriever using embeddings\n",
    "Instead of retrieving via Elasticsearch's plain BM25, we want to use vector similarity of the questions (user question vs. FAQ ones).\n",
    "We can use the `EmbeddingRetriever` for this purpose and specify a model that we use for the embeddings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "09/16/2020 02:24:19 - INFO - haystack.retriever.dense -   Init retriever using embeddings of model deepset/sentence_bert\n09/16/2020 02:24:19 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n09/16/2020 02:24:19 - INFO - farm.infer -   Could not find `deepset/sentence_bert` locally. Try to download from model hub ...\n09/16/2020 02:24:30 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n\t We guess it's an *ENGLISH* model ... \n\t If not: Init the language model by supplying the 'language' param.\n09/16/2020 02:24:31 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n"
    }
   ],
   "source": [
    "retriever = EmbeddingRetriever(document_store=document_store, embedding_model=\"deepset/sentence_bert\", use_gpu=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare & Index FAQ data\n",
    "We create a pandas dataframe containing some FAQ data (i.e curated pairs of question + answer) and index those in elasticsearch.\n",
    "Here: We download some question-answer pairs related to COVID-19"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Inferencing Samples: 100%|██████████| 25/25 [03:17<00:00,  7.91s/ Batches]\n09/16/2020 02:27:51 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:1.978s]\n"
    }
   ],
   "source": [
    "# Now, let's write the dicts containing documents to our DB.\n",
    "\n",
    "data['title'] = data['title'].apply(lambda x: x.strip())\n",
    "data = data.rename(columns = {'title': 'question', 'abstract': 'text'})\n",
    "data['question_emb'] = retriever.embed_queries(texts = list(data['question'].values))\n",
    "\n",
    "docs_to_index = data.to_dict(orient = 'records')\n",
    "document_store.write_documents(docs_to_index)\n",
    "# document_store.write_documents(data[['title', 'abstract']].rename(columns={'title':'name','abstract':'text'}).to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Download\n",
    "# temp = requests.get(\"https://raw.githubusercontent.com/deepset-ai/COVID-QA/master/data/faqs/faq_covidbert.csv\")\n",
    "# open('small_faq_covid.csv', 'wb').write(temp.content)\n",
    "\n",
    "# # Get dataframe with columns \"question\", \"answer\" and some custom metadata\n",
    "# df = pd.read_csv(\"small_faq_covid.csv\")\n",
    "# # Minimal cleaning\n",
    "# df.fillna(value=\"\", inplace=True)\n",
    "# df[\"question\"] = df[\"question\"].apply(lambda x: x.strip())\n",
    "# print(df.head())\n",
    "\n",
    "# # Get embeddings for our questions from the FAQs\n",
    "# questions = list(df[\"question\"].values)\n",
    "# df[\"question_emb\"] = retriever.embed_queries(texts=questions)\n",
    "# df[\"question_emb\"] = df[\"question_emb\"].apply(list) # convert from numpy to list for ES indexing\n",
    "# df = df.rename(columns={\"answer\": \"text\"})\n",
    "\n",
    "# # Convert Dataframe to list of dicts and index them in our DocumentStore\n",
    "# docs_to_index = df.to_dict(orient=\"records\")\n",
    "# document_store.write_documents(docs_to_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ask questions\n",
    "Initialize a Finder (this time without a reader) and ask questions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": " '$4500\\\\kms$ is easily obtained in the '\n                                  'relativistic MONDian lensing model of\\n'\n                                  'Angus et al. (2007). However, MONDian model '\n                                  'with little hot dark matter\\n'\n                                  '$M_{HDM} \\\\le 0.6\\\\times 10^{15}\\\\msun$ and '\n                                  'CDM model with a small halo mass $\\\\le\\n'\n                                  '1\\\\times 10^{15}\\\\msun$ are barely '\n                                  'consistent with lensing and velocity '\n                                  'data.\\n',\n                       'document_id': '0704.0094',\n                       'meta': {   'authors': 'HongSheng Zhao (SUPA, St '\n                                              'Andrews)',\n                                   'authors_parsed': [   [   'Zhao',\n                                                             'HongSheng',\n                                                             '',\n                                                             'SUPA, St '\n                                                             'Andrews']],\n                                   'categories': 'astro-ph',\n                                   'comments': '5-pages, Physical Review D, '\n                                               'rapid publication submitted',\n                                   'doi': None,\n                                   'journal-ref': None,\n                                   'license': None,\n                                   'question': 'Timing and Lensing of the '\n                                               'Colliding Bullet Clusters: '\n                                               'barely enough time\\n'\n                                               '  and gravity to accelerate '\n                                               'the bullet',\n                                   'report-no': None,\n                                   'submitter': 'HongSheng Zhao',\n                                   'update_date': '2007-05-23',\n                                   'versions': [   {   'created': 'Mon, 2 Apr '\n                                                                  '2007 '\n                                                                  '18:30:16 '\n                                                                  'GMT',\n                                                       'version': 'v1'}]},\n                       'offset_end': 1347,\n                       'offset_start': 0,\n                       'probability': 0.5560768,\n                       'question': None,\n                       'score': 0.11215360000000008},\n                   {   'answer': '  Possible (algebraic) commutation relations '\n                                 'in the Lagrangian quantum theory\\n'\n                                 'of free (scalar, spinor and vector) fields '\n                                 'are considered from mathematical\\n'\n                                 'view-point. As sources of these relations '\n                                 'are employed the Heisenberg\\n'\n                                 'equations/relations for the dynamical '\n                                 'variables and a specific condition for\\n'\n                                 'uniqueness of the operators of the dynamical '\n                                 'variables (with respect to some\\n'\n                                 'class of Lagrangians). The paracommutation '\n                                 'relations or some their\\n'\n                                 'generalizations are pointed as the most '\n                                 'general ones that entail the validity\\n'\n                                 'of all Heisenberg equations. The '\n                                 'simultaneous fulfillment of the Heisenberg\\n'\n                                 'equations and the uniqueness requirement '\n                                 'turn to be impossible. This problem is\\n'\n                                 'solved via a redefinition of the dynamical '\n                                 'variables, similar to the normal\\n'\n                                 'ordering procedure and containing it as a '\n                                 'special case. That implies\\n'\n                                 'corresponding changes in the admissible '\n                                 'commutation relations. The introduction\\n'\n                                 'of the concept of the vacuum makes narrow '\n                                 'the class of the possible commutation\\n'\n                                 'relations; in particular, the mentioned '\n                                 'redefinition of the dynamical variables\\n'\n                                 'is reduced to normal ordering. As a last '\n                                 'restriction on that class is imposed\\n'\n                                 'the requirement for existing of an effective '\n                                 'procedure for calculating vacuum\\n'\n                                 'mean values. The standard bilinear '\n                                 'commutation relations are pointed as the\\n'\n                                 'only known ones that satisfy all of the '\n                                 'mentioned conditions and do not\\n'\n                                 'contradict to the existing data.\\n',\n                       'context': '  Possible (algebraic) commutation '\n                                  'relations in the Lagrangian quantum theory\\n'\n                                  'of free (scalar, spinor and vector) fields '\n                                  'are considered from mathematical\\n'\n                                  'view-point. As sources of these relations '\n                                  'are employed the Heisenberg\\n'\n                                  'equations/relations for the dynamical '\n                                  'variables and a specific condition for\\n'\n                                  'uniqueness of the operators of the '\n                                  'dynamical variables (with respect to some\\n'\n                                  'class of Lagrangians). The paracommutation '\n                                  'relations or some their\\n'\n                                  'generalizations are pointed as the most '\n                                  'general ones that entail the validity\\n'\n                                  'of all Heisenberg equations. The '\n                                  'simultaneous fulfillment of the Heisenberg\\n'\n                                  'equations and the uniqueness requirement '\n                                  'turn to be impossible. This problem is\\n'\n                                  'solved via a redefinition of the dynamical '\n                                  'variables, similar to the normal\\n'\n                                  'ordering procedure and containing it as a '\n                                  'special case. That implies\\n'\n                                  'corresponding changes in the admissible '\n                                  'commutation relations. The introduction\\n'\n                                  'of the concept of the vacuum makes narrow '\n                                  'the class of the possible commutation\\n'\n                                  'relations; in particular, the mentioned '\n                                  'redefinition of the dynamical variables\\n'\n                                  'is reduced to normal ordering. As a last '\n                                  'restriction on that class is imposed\\n'\n                                  'the requirement for existing of an '\n                                  'effective procedure for calculating vacuum\\n'\n                                  'mean values. The standard bilinear '\n                                  'commutation relations are pointed as the\\n'\n                                  'only known ones that satisfy all of the '\n                                  'mentioned conditions and do not\\n'\n                                  'contradict to the existing data.\\n',\n                       'document_id': '0704.0066',\n                       'meta': {   'authors': 'Bozhidar Z. Iliev (Institute '\n                                              'for Nuclear Research and '\n                                              'Nuclear Energy,\\n'\n                                              '  Bulgarian Academy of '\n                                              'Sciences, Sofia, Bulgaria)',\n                                   'authors_parsed': [   [   'Iliev',\n                                                             'Bozhidar Z.',\n                                                             '',\n                                                             'Institute for '\n                                                             'Nuclear Research '\n                                                             'and Nuclear '\n                                                             'Energy,\\n'\n                                                             '  Bulgarian '\n                                                             'Academy of '\n                                                             'Sciences, Sofia, '\n                                                             'Bulgaria']],\n                                   'categories': 'hep-th',\n                                   'comments': '60 LaTeX pages. The packages '\n                                               'AMS-LaTeX and amsfonts are '\n                                               'required.\\n'\n                                               '  This paper is a continuation '\n                                               'of the e-print E-prints No. '\n                                               'hep-th/0402006, No.\\n'\n                                               '  hep-th/0405008 and No. '\n                                               'hep-th/0505007. For related '\n                                               'papers, visit the\\n'\n                                               '  \"publication\" pages at '\n                                               'http://theo.inrne.bas.bg/~bozho/',\n                                   'doi': None,\n                                   'journal-ref': None,\n                                   'license': None,\n                                   'question': 'Lagrangian quantum field '\n                                               'theory in momentum picture. '\n                                               'IV. Commutation\\n'\n                                               '  relations for free fields',\n                                   'report-no': None,\n                                   'submitter': 'Bozhidar Zakhariev Iliev',\n                                   'update_date': '2007-05-23',\n                                   'versions': [   {   'created': 'Sun, 1 Apr '\n                                                                  '2007 '\n                                                                  '06:22:38 '\n                                                                  'GMT',\n                                                       'version': 'v1'}]},\n                       'offset_end': 1401,\n                       'offset_start': 0,\n                       'probability': 0.5550638,\n                       'question': None,\n                       'score': 0.11012759999999999},\n                   {   'answer': '  We report on the analysis of selected '\n                                 'single source data sets from the first\\n'\n                                 'round of the Mock LISA Data Challenges '\n                                 '(MLDC) for white dwarf binaries. We\\n'\n                                 'implemented an end-to-end pipeline '\n                                 'consisting of a grid-based coherent\\n'\n                                 'pre-processing unit for signal detection, '\n                                 'and an automatic Markov Chain Monte\\n'\n                                 'Carlo post-processing unit for signal '\n                                 'evaluation. We demonstrate that signal\\n'\n                                 'detection with our coherent approach is '\n                                 'secure and accurate, and is increased\\n'\n                                 'in accuracy and supplemented with additional '\n                                 'information on the signal\\n'\n                                 'parameters by our Markov Chain Monte Carlo '\n                                 'approach. We also demonstrate that\\n'\n                                 'the Markov Chain Monte Carlo routine is '\n                                 'additionally able to determine\\n'\n                                 'accurately the noise level in the frequency '\n                                 'window of interest.\\n',\n                       'context': '  We report on the analysis of selected '\n                                  'single source data sets from the first\\n'\n                                  'round of the Mock LISA Data Challenges '\n                                  '(MLDC) for white dwarf binaries. We\\n'\n                                  'implemented an end-to-end pipeline '\n                                  'consisting of a grid-based coherent\\n'\n                                  'pre-processing unit for signal detection, '\n                                  'and an automatic Markov Chain Monte\\n'\n                                  'Carlo post-processing unit for signal '\n                                  'evaluation. We demonstrate that signal\\n'\n                                  'detection with our coherent approach is '\n                                  'secure and accurate, and is increased\\n'\n                                  'in accuracy and supplemented with '\n                                  'additional information on the signal\\n'\n                                  'parameters by our Markov Chain Monte Carlo '\n                                  'approach. We also demonstrate that\\n'\n                                  'the Markov Chain Monte Carlo routine is '\n                                  'additionally able to determine\\n'\n                                  'accurately the noise level in the frequency '\n                                  'window of interest.\\n',\n                       'document_id': '0704.0048',\n                       'meta': {   'authors': 'Alexander Stroeer, John Veitch, '\n                                              'Christian Roever, Ed Bloomer, '\n                                              'James\\n'\n                                              '  Clark, Nelson Christensen, '\n                                              'Martin Hendry, Chris Messenger, '\n                                              'Renate Meyer,\\n'\n                                              '  Matthew Pitkin, Jennifer '\n                                              'Toher, Richard Umstaetter, '\n                                              'Alberto Vecchio and\\n'\n                                              '  Graham Woan',\n                                   'authors_parsed': [   [   'Stroeer',\n                                                             'Alexander',\n                                                             ''],\n                                                         ['Veitch', 'John', ''],\n                                                         [   'Roever',\n                                                             'Christian',\n                                                             ''],\n                                                         ['Bloomer', 'Ed', ''],\n                                                         ['Clark', 'James', ''],\n                                                         [   'Christensen',\n                                                             'Nelson',\n                                                             ''],\n                                                         [   'Hendry',\n                                                             'Martin',\n                                                             ''],\n                                                         [   'Messenger',\n                                                             'Chris',\n                                                             ''],\n                                                         [   'Meyer',\n                                                             'Renate',\n                                                             ''],\n                                                         [   'Pitkin',\n                                                             'Matthew',\n                                                             ''],\n                                                         [   'Toher',\n                                                             'Jennifer',\n                                                             ''],\n                                                         [   'Umstaetter',\n                                                             'Richard',\n                                                             ''],\n                                                         [   'Vecchio',\n                                                             'Alberto',\n                                                             ''],\n                                                         [   'Woan',\n                                                             'Graham',\n                                                             '']],\n                                   'categories': 'gr-qc astro-ph',\n                                   'comments': 'GWDAW-11 proceeding, submitted '\n                                               'to CQG, 10 pages, 3 figures, 1 '\n                                               'table;\\n'\n                                               '  revised values in table',\n                                   'doi': '10.1088/0264-9381/24/19/S17',\n                                   'journal-ref': 'Class.Quant.Grav.24:S541-S550,2007',\n                                   'license': None,\n                                   'question': 'Inference on white dwarf '\n                                               'binary systems using the first '\n                                               'round Mock LISA\\n'\n                                               '  Data Challenges data sets',\n                                   'report-no': None,\n                                   'submitter': 'Alexander Stroeer',\n                                   'update_date': '2008-11-26',\n                                   'versions': [   {   'created': 'Sat, 31 Mar '\n                                                                  '2007 '\n                                                                  '19:17:47 '\n                                                                  'GMT',\n                                                       'version': 'v1'},\n                                                   {   'created': 'Tue, 3 Apr '\n                                                                  '2007 '\n                                                                  '22:26:37 '\n                                                                  'GMT',\n                                                       'version': 'v2'}]},\n                       'offset_end': 742,\n                       'offset_start': 0,\n                       'probability': 0.5539155,\n                       'question': None,\n                       'score': 0.10783100000000001}],\n    'question': 'How is the virus spreading?'}\n"
    }
   ],
   "source": [
    "finder = Finder(reader=None, retriever=retriever)\n",
    "prediction = finder.get_answers_via_similar_questions(question=\"Language models for chemestry\", top_k_retriever=10)\n",
    "print_answers(prediction, details=\"all\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "outputPrepend"
    ]
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}